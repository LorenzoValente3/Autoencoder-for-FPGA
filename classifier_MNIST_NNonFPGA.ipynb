{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import standard\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import preprocessing\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "#per trasformare un intero rapresentante una classe in un array one hot encoded\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#import per il modello\n",
    "from tensorflow.python.keras.engine.functional import Functional\n",
    "from tensorflow.python.keras import Input\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2,l1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definizione di funzioni utili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_nparray(model):\n",
    "    \n",
    "    weights = []\n",
    "    for layer in model.layers:\n",
    "        for M in layer.get_weights():\n",
    "            for W in M:\n",
    "                for w in W.flatten():\n",
    "                    weights.append(w)\n",
    "\n",
    "    #ora ho un ndarray con tutti i weights e bias\n",
    "    weights = np.array(weights)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def pruning(model, soglia_percentile = 100/33, minimum_weight = 2**-5):\n",
    "\n",
    "    model_config = model.get_config()\n",
    "    model_pruned = Sequential.from_config(model_config)\n",
    "    \n",
    "    \n",
    "    weights = get_weights_nparray(model)\n",
    "    \n",
    "    total_weights = model.count_params()\n",
    "    non_zero_weight = len(weights[weights != 0])\n",
    "    print('initial non zero weights: ', non_zero_weight, ' su: ', total_weights)\n",
    "    \n",
    "    absolute_max = np.absolute(weights.max())\n",
    "    #weights = np.absolute(weights/absolute_max)\n",
    "    \n",
    "    print('\\npeso massimo: ',absolute_max)\n",
    "    \n",
    "    soglia = np.percentile(np.absolute(weights[weights != 0]), soglia_percentile)\n",
    "    print('\\nsoglia percentile: ',soglia, '\\tsoglia min W: ', minimum_weight)\n",
    "    if soglia < minimum_weight:\n",
    "        soglia = minimum_weight\n",
    "    \n",
    "    print('\\nsoglia: ',soglia)\n",
    "    \n",
    "    \n",
    "    list_M_pruned = []\n",
    "    \n",
    "    for M in model.get_weights():\n",
    "        #index_to_prune = np.logical_not(np.all([M < soglia, M > -soglia],axis=0))\n",
    "        index_to_prune = np.all([M < soglia, M > -soglia],axis=0)\n",
    "    \n",
    "        M_pruned = np.copy(M)\n",
    "    \n",
    "        M_pruned[index_to_prune] = 0\n",
    "    \n",
    "        list_M_pruned.append(M_pruned)\n",
    "    \n",
    "    model_pruned.set_weights(list_M_pruned)\n",
    "    \n",
    "    weights = get_weights_nparray(model_pruned)\n",
    "    non_zero_weight = len(weights[weights != 0])\n",
    "    print('\\nafter pruning non zero weights: ', non_zero_weight, ' su: ', total_weights)\n",
    "    \n",
    "    return model_pruned\n",
    "\n",
    "def save_model(model, name):\n",
    "\n",
    "    model.save('complete_model_' + name + '.h5')\n",
    "\n",
    "    model_json_string = model.to_json()\n",
    "    with open('model_' + name + '.json', 'w') as f:\n",
    "        f.write(model_json_string)\n",
    "    \n",
    "    model.save_weights('model_weights_' + name + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caricamento del dataset e preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download mnist data and split into train and test sets\n",
    "(X_train, y_train_int), (X_test, y_test_int) = mnist.load_data()\n",
    "\n",
    "#one-hot encode target column\n",
    "y_train = to_categorical(y_train_int)\n",
    "y_test = to_categorical(y_test_int)\n",
    "\n",
    "X_train[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genero le immagini flat and zoom a 5-bit di color depth in scala di grigio\n",
    "\n",
    "#-----USER DEFINED VALUES--------------------------- \n",
    "\n",
    "#dimensione taglio iniziale delle immagini originali che sono 28. \n",
    "#Questa procedura è un male necessario, vi è un tradeoff fra questa e size_final\n",
    "size_initial = 20\n",
    "\n",
    "#dimensione finale delle immagini\n",
    "size_final = 8\n",
    "\n",
    "#profondità pixel in bit\n",
    "color_depth = 5\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "X_train_flat_zoom = []\n",
    "X_test_flat_zoom = []\n",
    "\n",
    "X_train_flat_zoom_int = []\n",
    "X_test_flat_zoom_int = []\n",
    "\n",
    "#defisce gli indici del cropping definito da size_initial\n",
    "bordo = (28-size_initial)//2\n",
    "bordo_top = -bordo\n",
    "if bordo == 0:\n",
    "    bordo_top = None\n",
    "\n",
    "    \n",
    "#processing Training Set\n",
    "for image in X_train:\n",
    "    tmp = scipy.ndimage.zoom(image[bordo:bordo_top, bordo:bordo_top], size_final/size_initial).flatten()\n",
    "    tmp = (tmp/(256//2**color_depth)).astype(int)\n",
    "    X_train_flat_zoom.append(tmp/2**color_depth)\n",
    "    X_train_flat_zoom_int.append(tmp)\n",
    "\n",
    "#processing Test Set\n",
    "for image in X_test:\n",
    "    tmp = scipy.ndimage.zoom(image[bordo:bordo_top, bordo:bordo_top], size_final/size_initial).flatten()\n",
    "    tmp = (tmp/(256//2**color_depth)).astype(int)\n",
    "    X_test_flat_zoom.append(tmp/2**color_depth)\n",
    "    X_test_flat_zoom_int.append(tmp)\n",
    "    \n",
    "X_train_flat_zoom = np.array(X_train_flat_zoom)\n",
    "X_test_flat_zoom = np.array(X_test_flat_zoom)\n",
    "\n",
    "X_train_flat_zoom_int = np.array(X_train_flat_zoom_int)\n",
    "X_test_flat_zoom_int = np.array(X_test_flat_zoom_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 64)\n",
      "int32\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(X_test_flat_zoom_int[0].reshape((size_final,size_final)), cmap='gray')  \n",
    "\n",
    "print (X_train_flat_zoom.shape)\n",
    "print (X_train_flat_zoom_int.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                2112      \n",
      "=================================================================\n",
      "Total params: 4,354\n",
      "Trainable params: 4,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#ENCODER\n",
    "input = Input(shape=(X_train_flat_zoom_int.shape[-1],))\n",
    "encoder = Dense(32, activation='relu')(input)\n",
    "encoder = Dense(2, activation='relu')(encoder)\n",
    "\n",
    "#DECODER\n",
    "decoder = Dense(32, activation='relu')(encoder)\n",
    "decoder = Dense(64, activation='sigmoid')(decoder)\n",
    "\n",
    "#AUTOENCODER\n",
    "autoencoder = Model ( input, outputs=decoder)\n",
    "autoencoder.compile(loss='mse', optimizer='adam')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1300 - val_loss: 0.1014\n",
      "Epoch 2/20\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0979 - val_loss: 0.0946\n",
      "Epoch 3/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0930 - val_loss: 0.0911\n",
      "Epoch 4/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0903 - val_loss: 0.0891\n",
      "Epoch 5/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0885 - val_loss: 0.0876\n",
      "Epoch 6/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0871 - val_loss: 0.0862\n",
      "Epoch 7/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0856 - val_loss: 0.0847\n",
      "Epoch 8/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0842 - val_loss: 0.0832\n",
      "Epoch 9/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0829 - val_loss: 0.0820\n",
      "Epoch 10/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0817 - val_loss: 0.0810\n",
      "Epoch 11/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0807 - val_loss: 0.0802\n",
      "Epoch 12/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0800 - val_loss: 0.0797\n",
      "Epoch 13/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0794 - val_loss: 0.0792\n",
      "Epoch 14/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0790 - val_loss: 0.0788\n",
      "Epoch 15/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0786 - val_loss: 0.0784\n",
      "Epoch 16/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0782 - val_loss: 0.0780\n",
      "Epoch 17/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0779 - val_loss: 0.0777\n",
      "Epoch 18/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0776 - val_loss: 0.0774\n",
      "Epoch 19/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0772 - val_loss: 0.0770\n",
      "Epoch 20/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0768 - val_loss: 0.0766\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(X_train_flat_zoom, X_train_flat_zoom,\n",
    "                            validation_data=(X_test_flat_zoom, X_test_flat_zoom),\n",
    "                            batch_size = 256, epochs = 20,\n",
    "                            shuffle = True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definizione dell'architettura NN e addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 2,778\n",
      "Trainable params: 2,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_flat_zoom = Sequential([\n",
    "    Dense(32, input_shape=(X_train_flat_zoom.shape[-1],), activation='relu', kernel_regularizer=l2(0.01)),  \n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(10, activation='softmax', kernel_regularizer=l2(0.01))\n",
    "                        ])\n",
    "\n",
    "model_flat_zoom.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 1.4470 - acc: 0.7483 - val_loss: 0.6672 - val_acc: 0.8870\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.5714 - acc: 0.8990 - val_loss: 0.4732 - val_acc: 0.92015890 - acc: 0 - ETA: 0s - loss: 0.5767 - acc: 0.89\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4650 - acc: 0.9176 - val_loss: 0.4244 - val_acc: 0.9262\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4307 - acc: 0.9260 - val_loss: 0.4236 - val_acc: 0.9271\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4179 - acc: 0.9283 - val_loss: 0.3962 - val_acc: 0.9353 0s - loss: 0.4158 - acc\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4082 - acc: 0.9316 - val_loss: 0.3996 - val_acc: 0.9331\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.4029 - acc: 0.9333 - val_loss: 0.3845 - val_acc: 0.9406 - loss: 0.4044 - acc: 0.\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3989 - acc: 0.9345 - val_loss: 0.3929 - val_acc: 0.9356s - loss: 0.3976 - acc:\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3968 - acc: 0.9359 - val_loss: 0.3857 - val_acc: 0.9389\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3939 - acc: 0.9363 - val_loss: 0.3775 - val_acc: 0.9442loss: 0.3937 - acc\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3922 - acc: 0.9375 - val_loss: 0.3861 - val_acc: 0.9387\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3904 - acc: 0.9387 - val_loss: 0.3730 - val_acc: 0.9453\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3889 - acc: 0.9385 - val_loss: 0.3954 - val_acc: 0.9364\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.3878 - acc: 0.9404- ETA: 1s - lo - 2s 32us/sample - loss: 0.3879 - acc: 0.9404 - val_loss: 0.3697 - val_acc: 0.9452\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3882 - acc: 0.9389 - val_loss: 0.3705 - val_acc: 0.9471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17da3a374a8>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epoche di training\n",
    "training_epoch = 15\n",
    "\n",
    "#compilazione del modello\n",
    "model_flat_zoom.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#training del modello\n",
    "history = model_flat_zoom.fit(x=X_train_flat_zoom_int, y=y_train, validation_data=(X_test_flat_zoom_int, y_test), epochs=training_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning del modello e verifica dell'accuratezza sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial non zero weights:  2778  su:  2778\n",
      "\n",
      "peso massimo:  0.972445\n",
      "\n",
      "soglia percentile:  0.0005083537773426735 \tsoglia min W:  0.03125\n",
      "\n",
      "soglia:  0.03125\n",
      "\n",
      "after pruning non zero weights:  1527  su:  2778\n"
     ]
    }
   ],
   "source": [
    "model_flat_zoom_pruned = pruning(model_flat_zoom, minimum_weight=2**-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in log2\n",
      "  \n",
      "C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:824: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:825: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([85., 72., 77., 90., 70., 79., 61., 51., 48., 37., 33., 27.,  9.,\n",
       "        13.,  6.,  5.,  7.,  5.,  1.,  2.]),\n",
       " array([-4.999821  , -4.751846  , -4.50387   , -4.2558947 , -4.0079193 ,\n",
       "        -3.7599437 , -3.5119684 , -3.2639928 , -3.0160172 , -2.7680418 ,\n",
       "        -2.5200663 , -2.272091  , -2.0241153 , -1.7761399 , -1.5281644 ,\n",
       "        -1.2801889 , -1.0322133 , -0.78423786, -0.5362624 , -0.28828692,\n",
       "        -0.04031142], dtype=float32),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_flat_zoom_pruned = np.absolute(get_weights_nparray(model_flat_zoom_pruned))\n",
    "plt.hist(np.log2(w_flat_zoom_pruned[w_flat_zoom_pruned!=0]), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss   Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38490096, 0.9423]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_flat_zoom_pruned.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print('    Loss   Accuracy')\n",
    "model_flat_zoom_pruned.test_on_batch(x=X_test_flat_zoom_int, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvataggio del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'MNIST_flat' + str(size_final) + '_' + str(color_depth) + 'bit'\n",
    "\n",
    "save_model(model_flat_zoom, name_model)\n",
    "save_model(model_flat_zoom_pruned, name_model + '_pruned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvataggio delle immagini test da inserire nella FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista contenente gli indici delle immagini campione nel Test Set del MNIST\n",
    "#num_list[0] contiene l'indice dell'immagine di uno zero, num_list[1] di un uno e così via\n",
    "#           0  1  2  3  4  5  6  7  8  9          \n",
    "num_list = [3, 2, 1, 32,4, 15,21,0, 61,12]\n",
    "\n",
    "\n",
    "#---------------------------------------------------------\n",
    "#--FORMAT OF INPUT IMAGES FOR NN IN FPGA------------------\n",
    "#----------- ap_fixed<7,2> -------------------------------\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# last pixel                 first pixel\n",
    "#[0  0  1  0  0  0  0 | ... | 0  0  0  1  0  0  0]\n",
    "# s  0 -1 -2 -3 -4 -5         s  0 -1 -2 -3 -4 -5 \n",
    "#    2  2  2  2  2  2            2  2  2  2  2  2\n",
    "\n",
    "with open('TESTimg_downto_apfixed7_2.txt', 'w') as f:\n",
    "    for index , num in enumerate(num_list):\n",
    "        f.write(str(index)+ ': ')\n",
    "        for i in np.flip(X_test_flat_zoom_int[num]):\n",
    "            f.write('{:07d}'.format(int((bin(i)[2:]))))\n",
    "        f.write('\\n\\n')\n",
    "        \n",
    "        \n",
    "#---------------------------------------------------------\n",
    "#--FORMAT OF INPUT IMAGES FOR NN IN FPGA------------------\n",
    "#----------- ap_int<7,2> -------------------------------\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# last pixel              first pixel\n",
    "#[0  1  0  0  0  0 | ... | 0  0  0  1  0  0]\n",
    "# s  4  3  2  1  0         s  4  3  2  1  0 \n",
    "#    2  2  2  2  2            2  2  2  2  2  \n",
    "        \n",
    "with open('TESTimg_downto_apint6.txt', 'w') as f:\n",
    "    for index , num in enumerate(num_list):\n",
    "        f.write(str(index)+ ': ')\n",
    "        for i in np.flip(X_test_flat_zoom_int[num]):\n",
    "            f.write('{:06d}'.format(int((bin(i)[2:]))))\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info dettagliate delle inferenze sulle immagini test inserite nella FPGA. \n",
    "Confronto fra modello completo e prunato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predizioni del modello NON PRUNATO sulle immagini di test scelte\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_9_input to have shape (64,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-f6a3c7be0bdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predizioni del modello NON PRUNATO sulle immagini di test scelte\\n\\n\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'number in image: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\nPrediction: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_flat_zoom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_flat_zoom\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[1;31m# generate symbolic tensors).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1060\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m   1061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2651\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    383\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    386\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_9_input to have shape (64,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "print('predizioni del modello NON PRUNATO sulle immagini di test scelte\\n\\n\\n')\n",
    "for i,index in enumerate(num_list):\n",
    "    print('number in image: ', i,'\\nPrediction: ',model_flat_zoom.predict(X_test_flat_zoom_int[index].reshape(1,-1)))\n",
    "    print('\\n')\n",
    "\n",
    "print('predizioni del modello PRUNATO sulle immagini di test scelte\\n\\n\\n')\n",
    "for i,index in enumerate(num_list):\n",
    "    print('number in image: ', i,'\\nPrediction: ',model_flat_zoom_pruned.predict(X_test_flat_zoom_int[index].reshape(1,-1)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni utili per la scrittura del codice vhdl di input.\n",
    "\n",
    "Particolarmente verboso e quindi utile generarlo automaticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista contenente gli indici delle immagini campione nel Test Set del MNIST\n",
    "#num_list[0] contiene l'indice dell'immagine di uno zero, num_list[1] di un uno e così via\n",
    "\n",
    "n_image_test = 100\n",
    "\n",
    "#           0  1  2  3  4  5  6  7  8  9          \n",
    "num_list = [3, 2, 1, 32,4, 15,21,0, 61,12] + [x for x in range(62,62 + n_image_test - 10)]\n",
    "\n",
    "\n",
    "#---------------------------------------------------------\n",
    "#--FORMAT OF INPUT IMAGES FOR NN IN FPGA------------------\n",
    "#----------- ap_fixed<7,2> -------------------------------\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# last pixel                 first pixel\n",
    "#[0  0  1  0  0  0  0 | ... | 0  0  0  1  0  0  0]\n",
    "# s  0 -1 -2 -3 -4 -5         s  0 -1 -2 -3 -4 -5 \n",
    "#    2  2  2  2  2  2            2  2  2  2  2  2\n",
    "\n",
    "'''\n",
    "with open('TESTimg_downto_apfixed7_2.txt', 'w') as f:\n",
    "    for index , num in enumerate(num_list):\n",
    "        f.write(str(index)+ ': ')\n",
    "        for i in np.flip(X_test_flat_zoom_int[num]):\n",
    "            f.write('{:07d}'.format(int((bin(i)[2:]))))\n",
    "        f.write('\\n\\n')\n",
    "'''\n",
    "        \n",
    "#---------------------------------------------------------\n",
    "#--FORMAT OF INPUT IMAGES FOR NN IN FPGA------------------\n",
    "#----------- ap_int<7,2> -------------------------------\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# last pixel              first pixel\n",
    "#[0  1  0  0  0  0 | ... | 0  0  0  1  0  0]\n",
    "# s  4  3  2  1  0         s  4  3  2  1  0 \n",
    "#    2  2  2  2  2            2  2  2  2  2  \n",
    "\n",
    "with open('TESTimg_downto_apint6_100imgs.txt', 'w') as f:\n",
    "    for index , num in enumerate(num_list):\n",
    "        f.write('signal address_' + str(index) + ' : std_logic_vector((size-1) downto 0) := \\\"')\n",
    "        for i in np.flip(X_test_flat_zoom_int[num]):\n",
    "            f.write('{:06d}'.format(int((bin(i)[2:]))))\n",
    "        f.write('\\\";\\n')\n",
    "        \n",
    "\n",
    "#salvataggio dei true label associati alle immagini inserite nella FPGA\n",
    "with open('TESTlabels_downto_apint6_100imgs.txt', 'w') as f:\n",
    "    for index , num in enumerate(num_list):\n",
    "        f.write('signal label_' + str(index) + ' : std_logic_vector(3 downto 0) := \\\"')\n",
    "        f.write('{:04d}'.format(int((bin(y_test_int[num])[2:]))))\n",
    "        f.write('\\\";\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_image_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-96e59c068904>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vhdl_code_1.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'flat_image <= '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_image_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'address_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' when (address = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m') else\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'(others => \\'0\\');'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_image_test' is not defined"
     ]
    }
   ],
   "source": [
    "#generazione di codice vhdl utile per il core di generazione di input nella FPGA\n",
    "\n",
    "with open('vhdl_code_1.txt', 'w') as f:\n",
    "    f.write('flat_image <= ')\n",
    "    for i in range(n_image_test):\n",
    "        f.write('address_' + str(i) + ' when (address = ' + str(i) + ') else\\n')\n",
    "    f.write('(others => \\'0\\');')\n",
    "    \n",
    "with open('vhdl_code_2.txt', 'w') as f:\n",
    "    f.write('true_label <= ')\n",
    "    for i in range(n_image_test):\n",
    "        f.write('label_' + str(i) + ' when (address = ' + str(i) + ') else\\n')\n",
    "    f.write('(others => \\'0\\');')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se si ha già il modello esportato e lo si vuole caricare per fare alcuni test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "load_model = load_model('complete_model_MNIST_flat8_5bit_pruned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34578407, 0.97]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuratezza del modello sulle immagini inserite nella FPGA\n",
    "\n",
    "load_model.test_on_batch(x=X_test_flat_zoom_int[num_list], y=y_test[num_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0019826889038085938 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Calcolo della latenza o tempo di inferenza sulla propria CPU, utilizzando le routine di Keras\n",
    "\n",
    "import time\n",
    "\n",
    "image  = X_test_flat_zoom_int[0].reshape(1,-1)\n",
    "start_time = time.time()\n",
    "\n",
    "load_model.predict(image)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
