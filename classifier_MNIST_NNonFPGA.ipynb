{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import standard\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import preprocessing\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "#per trasformare un intero rapresentante una classe in un array one hot encoded\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#import per il modello\n",
    "from tensorflow.python.keras.engine.functional import Functional\n",
    "from tensorflow.python.keras import Input\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2,l1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definizione di funzioni utili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_nparray(model):\n",
    "    \n",
    "    weights = []\n",
    "    for layer in model.layers:\n",
    "        for M in layer.get_weights():\n",
    "            for W in M:\n",
    "                for w in W.flatten():\n",
    "                    weights.append(w)\n",
    "\n",
    "    #ora ho un ndarray con tutti i weights e bias\n",
    "    weights = np.array(weights)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def pruning(model, soglia_percentile = 100/33, minimum_weight = 2**-5):\n",
    "\n",
    "    model_config = model.get_config()\n",
    "    model_pruned = Sequential.from_config(model_config)\n",
    "    \n",
    "    \n",
    "    weights = get_weights_nparray(model)\n",
    "    \n",
    "    total_weights = model.count_params()\n",
    "    non_zero_weight = len(weights[weights != 0])\n",
    "    print('initial non zero weights: ', non_zero_weight, ' su: ', total_weights)\n",
    "    \n",
    "    absolute_max = np.absolute(weights.max())\n",
    "    #weights = np.absolute(weights/absolute_max)\n",
    "    \n",
    "    print('\\npeso massimo: ',absolute_max)\n",
    "    \n",
    "    soglia = np.percentile(np.absolute(weights[weights != 0]), soglia_percentile)\n",
    "    print('\\nsoglia percentile: ',soglia, '\\tsoglia min W: ', minimum_weight)\n",
    "    if soglia < minimum_weight:\n",
    "        soglia = minimum_weight\n",
    "    \n",
    "    print('\\nsoglia: ',soglia)\n",
    "    \n",
    "    \n",
    "    list_M_pruned = []\n",
    "    \n",
    "    for M in model.get_weights():\n",
    "        #index_to_prune = np.logical_not(np.all([M < soglia, M > -soglia],axis=0))\n",
    "        index_to_prune = np.all([M < soglia, M > -soglia],axis=0)\n",
    "    \n",
    "        M_pruned = np.copy(M)\n",
    "    \n",
    "        M_pruned[index_to_prune] = 0\n",
    "    \n",
    "        list_M_pruned.append(M_pruned)\n",
    "    \n",
    "    model_pruned.set_weights(list_M_pruned)\n",
    "    \n",
    "    weights = get_weights_nparray(model_pruned)\n",
    "    non_zero_weight = len(weights[weights != 0])\n",
    "    print('\\nafter pruning non zero weights: ', non_zero_weight, ' su: ', total_weights)\n",
    "    \n",
    "    return model_pruned\n",
    "\n",
    "def save_model(model, name):\n",
    "\n",
    "    model.save('complete_model_' + name + '.h5')\n",
    "\n",
    "    model_json_string = model.to_json()\n",
    "    with open('model_' + name + '.json', 'w') as f:\n",
    "        f.write(model_json_string)\n",
    "    \n",
    "    model.save_weights('model_weights_' + name + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caricamento del dataset e preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download mnist data and split into train and test sets\n",
    "(X_train, y_train_int), (X_test, y_test_int) = mnist.load_data()\n",
    "\n",
    "#one-hot encode target column\n",
    "y_train = to_categorical(y_train_int)\n",
    "y_test = to_categorical(y_test_int)\n",
    "\n",
    "X_train[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genero le immagini flat and zoom a 5-bit di color depth in scala di grigio\n",
    "\n",
    "#-----USER DEFINED VALUES--------------------------- \n",
    "\n",
    "#dimensione taglio iniziale delle immagini originali che sono 28. \n",
    "#Questa procedura è un male necessario, vi è un tradeoff fra questa e size_final\n",
    "size_initial = 20\n",
    "\n",
    "#dimensione finale delle immagini\n",
    "size_final = 8\n",
    "\n",
    "#profondità pixel in bit\n",
    "color_depth = 5\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "X_train_flat_zoom = []\n",
    "X_test_flat_zoom = []\n",
    "\n",
    "X_train_flat_zoom_int = []\n",
    "X_test_flat_zoom_int = []\n",
    "\n",
    "#defisce gli indici del cropping definito da size_initial\n",
    "bordo = (28-size_initial)//2\n",
    "bordo_top = -bordo\n",
    "if bordo == 0:\n",
    "    bordo_top = None\n",
    "\n",
    "    \n",
    "#processing Training Set\n",
    "for image in X_train:\n",
    "    tmp = scipy.ndimage.zoom(image[bordo:bordo_top, bordo:bordo_top], size_final/size_initial).flatten()\n",
    "    tmp = (tmp/(256//2**color_depth)).astype(int)\n",
    "    X_train_flat_zoom.append(tmp/2**color_depth)\n",
    "    X_train_flat_zoom_int.append(tmp)\n",
    "\n",
    "#processing Test Set\n",
    "for image in X_test:\n",
    "    tmp = scipy.ndimage.zoom(image[bordo:bordo_top, bordo:bordo_top], size_final/size_initial).flatten()\n",
    "    tmp = (tmp/(256//2**color_depth)).astype(int)\n",
    "    X_test_flat_zoom.append(tmp/2**color_depth)\n",
    "    X_test_flat_zoom_int.append(tmp)\n",
    "    \n",
    "X_train_flat_zoom = np.array(X_train_flat_zoom)\n",
    "X_test_flat_zoom = np.array(X_test_flat_zoom)\n",
    "\n",
    "X_train_flat_zoom_int = np.array(X_train_flat_zoom_int)\n",
    "X_test_flat_zoom_int = np.array(X_test_flat_zoom_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 64)\n",
      "float64\n",
      "int32\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(X_test_flat_zoom_int[0].reshape((size_final,size_final)), cmap='gray')  \n",
    "\n",
    "print (X_train_flat_zoom.shape)\n",
    "X_train_flat_zoom.astype(int)\n",
    "print (X_train_flat_zoom.dtype)\n",
    "print (X_train_flat_zoom_int.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 32)           2080        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            66          dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           96          dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 64)           2112        dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 10)           30          dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,384\n",
      "Trainable params: 4,384\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#ENCODER\n",
    "input = Input(shape=(X_train_flat_zoom_int.shape[-1],))\n",
    "encoder = Dense(32, activation='relu')(input)\n",
    "encoder = Dense(2, activation='relu')(encoder)\n",
    "\n",
    "#DECODER\n",
    "decoder = Dense(32, activation='relu')(encoder)\n",
    "decoder = Dense(64, activation='sigmoid')(decoder)\n",
    "\n",
    "classifier = Dense(10, activation='softmax')(encoder)\n",
    "\n",
    "#AUTOENCODER\n",
    "autoencoder = Model ( input, outputs=[decoder, classifier])\n",
    "autoencoder.compile(loss=['mse', 'categorical_crossentropy'], optimizer='adam', metrics=\"accuracy\")\n",
    "autoencoder.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 161.1342 - dense_15_loss: 160.1447 - dense_16_loss: 0.9895 - dense_15_accuracy: 0.0440 - dense_16_accuracy: 0.6889 - val_loss: 163.9897 - val_dense_15_loss: 163.0452 - val_dense_16_loss: 0.9445 - val_dense_15_accuracy: 0.0424 - val_dense_16_accuracy: 0.7163\n",
      "Epoch 2/20\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 161.0748 - dense_15_loss: 160.1432 - dense_16_loss: 0.9317 - dense_15_accuracy: 0.0462 - dense_16_accuracy: 0.7071 - val_loss: 163.9355 - val_dense_15_loss: 163.0422 - val_dense_16_loss: 0.8933 - val_dense_15_accuracy: 0.0363 - val_dense_16_accuracy: 0.7235\n",
      "Epoch 3/20\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 161.0358 - dense_15_loss: 160.1414 - dense_16_loss: 0.8944 - dense_15_accuracy: 0.0457 - dense_16_accuracy: 0.7250 - val_loss: 163.9038 - val_dense_15_loss: 163.0399 - val_dense_16_loss: 0.8639 - val_dense_15_accuracy: 0.0519 - val_dense_16_accuracy: 0.7453\n",
      "Epoch 4/20\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 161.0087 - dense_15_loss: 160.1392 - dense_16_loss: 0.8694 - dense_15_accuracy: 0.0484 - dense_16_accuracy: 0.7365 - val_loss: 163.8927 - val_dense_15_loss: 163.0401 - val_dense_16_loss: 0.8526 - val_dense_15_accuracy: 0.0420 - val_dense_16_accuracy: 0.7396\n",
      "Epoch 5/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.9796 - dense_15_loss: 160.1385 - dense_16_loss: 0.8412 - dense_15_accuracy: 0.0507 - dense_16_accuracy: 0.7481 - val_loss: 163.8545 - val_dense_15_loss: 163.0382 - val_dense_16_loss: 0.8163 - val_dense_15_accuracy: 0.0612 - val_dense_16_accuracy: 0.7613\n",
      "Epoch 6/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.9544 - dense_15_loss: 160.1381 - dense_16_loss: 0.8163 - dense_15_accuracy: 0.0501 - dense_16_accuracy: 0.7603 - val_loss: 163.8371 - val_dense_15_loss: 163.0368 - val_dense_16_loss: 0.8003 - val_dense_15_accuracy: 0.0537 - val_dense_16_accuracy: 0.7755\n",
      "Epoch 7/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.9308 - dense_15_loss: 160.1371 - dense_16_loss: 0.7936 - dense_15_accuracy: 0.0522 - dense_16_accuracy: 0.7733 - val_loss: 163.8097 - val_dense_15_loss: 163.0361 - val_dense_16_loss: 0.7736 - val_dense_15_accuracy: 0.0545 - val_dense_16_accuracy: 0.7941\n",
      "Epoch 8/20\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 160.9066 - dense_15_loss: 160.1362 - dense_16_loss: 0.7704 - dense_15_accuracy: 0.0520 - dense_16_accuracy: 0.7858 - val_loss: 163.7899 - val_dense_15_loss: 163.0376 - val_dense_16_loss: 0.7523 - val_dense_15_accuracy: 0.0507 - val_dense_16_accuracy: 0.7982\n",
      "Epoch 9/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.8864 - dense_15_loss: 160.1351 - dense_16_loss: 0.7513 - dense_15_accuracy: 0.0514 - dense_16_accuracy: 0.7936 - val_loss: 163.7807 - val_dense_15_loss: 163.0349 - val_dense_16_loss: 0.7457 - val_dense_15_accuracy: 0.0581 - val_dense_16_accuracy: 0.8023\n",
      "Epoch 10/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.8687 - dense_15_loss: 160.1336 - dense_16_loss: 0.7351 - dense_15_accuracy: 0.0511 - dense_16_accuracy: 0.8003 - val_loss: 163.7611 - val_dense_15_loss: 163.0353 - val_dense_16_loss: 0.7258 - val_dense_15_accuracy: 0.0387 - val_dense_16_accuracy: 0.8109\n",
      "Epoch 11/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.8526 - dense_15_loss: 160.1332 - dense_16_loss: 0.7194 - dense_15_accuracy: 0.0491 - dense_16_accuracy: 0.8044 - val_loss: 163.7469 - val_dense_15_loss: 163.0332 - val_dense_16_loss: 0.7136 - val_dense_15_accuracy: 0.0440 - val_dense_16_accuracy: 0.8144\n",
      "Epoch 12/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.8401 - dense_15_loss: 160.1325 - dense_16_loss: 0.7075 - dense_15_accuracy: 0.0484 - dense_16_accuracy: 0.8081 - val_loss: 163.7413 - val_dense_15_loss: 163.0341 - val_dense_16_loss: 0.7073 - val_dense_15_accuracy: 0.0584 - val_dense_16_accuracy: 0.8160\n",
      "Epoch 13/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.8265 - dense_15_loss: 160.1329 - dense_16_loss: 0.6936 - dense_15_accuracy: 0.0454 - dense_16_accuracy: 0.8127 - val_loss: 163.7271 - val_dense_15_loss: 163.0355 - val_dense_16_loss: 0.6916 - val_dense_15_accuracy: 0.0568 - val_dense_16_accuracy: 0.8252\n",
      "Epoch 14/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.8136 - dense_15_loss: 160.1323 - dense_16_loss: 0.6814 - dense_15_accuracy: 0.0459 - dense_16_accuracy: 0.8181 - val_loss: 163.7132 - val_dense_15_loss: 163.0325 - val_dense_16_loss: 0.6807 - val_dense_15_accuracy: 0.0570 - val_dense_16_accuracy: 0.8234\n",
      "Epoch 15/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.8026 - dense_15_loss: 160.1317 - dense_16_loss: 0.6710 - dense_15_accuracy: 0.0444 - dense_16_accuracy: 0.8216 - val_loss: 163.7051 - val_dense_15_loss: 163.0324 - val_dense_16_loss: 0.6727 - val_dense_15_accuracy: 0.0455 - val_dense_16_accuracy: 0.8342\n",
      "Epoch 16/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.7934 - dense_15_loss: 160.1319 - dense_16_loss: 0.6617 - dense_15_accuracy: 0.0430 - dense_16_accuracy: 0.8256 - val_loss: 163.6941 - val_dense_15_loss: 163.0321 - val_dense_16_loss: 0.6620 - val_dense_15_accuracy: 0.0413 - val_dense_16_accuracy: 0.8364\n",
      "Epoch 17/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.7839 - dense_15_loss: 160.1314 - dense_16_loss: 0.6524 - dense_15_accuracy: 0.0452 - dense_16_accuracy: 0.8283 - val_loss: 163.6871 - val_dense_15_loss: 163.0316 - val_dense_16_loss: 0.6556 - val_dense_15_accuracy: 0.0494 - val_dense_16_accuracy: 0.8420\n",
      "Epoch 18/20\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 160.7731 - dense_15_loss: 160.1313 - dense_16_loss: 0.6418 - dense_15_accuracy: 0.0436 - dense_16_accuracy: 0.8337 - val_loss: 163.6792 - val_dense_15_loss: 163.0312 - val_dense_16_loss: 0.6480 - val_dense_15_accuracy: 0.0495 - val_dense_16_accuracy: 0.8445\n",
      "Epoch 19/20\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 160.7650 - dense_15_loss: 160.1310 - dense_16_loss: 0.6340 - dense_15_accuracy: 0.0439 - dense_16_accuracy: 0.8368 - val_loss: 163.6736 - val_dense_15_loss: 163.0307 - val_dense_16_loss: 0.6429 - val_dense_15_accuracy: 0.0445 - val_dense_16_accuracy: 0.8441\n",
      "Epoch 20/20\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 160.7585 - dense_15_loss: 160.1313 - dense_16_loss: 0.6273 - dense_15_accuracy: 0.0431 - dense_16_accuracy: 0.8393 - val_loss: 163.6695 - val_dense_15_loss: 163.0303 - val_dense_16_loss: 0.6392 - val_dense_15_accuracy: 0.0461 - val_dense_16_accuracy: 0.8465\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(X_train_flat_zoom_int, [X_train_flat_zoom_int, y_train],\n",
    "                            validation_data=(X_test_flat_zoom_int, [X_test_flat_zoom_int, y_test]),\n",
    "                            batch_size = 256, epochs = 20,\n",
    "                            shuffle = True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definizione dell'architettura NN e addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 2,778\n",
      "Trainable params: 2,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_flat_zoom = Sequential([\n",
    "    Dense(32, input_shape=(X_train_flat_zoom.shape[-1],), activation='relu', kernel_regularizer=l2(0.01)),  \n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(10, activation='softmax', kernel_regularizer=l2(0.01))\n",
    "                        ])\n",
    "\n",
    "model_flat_zoom.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.3421 - accuracy: 0.7185 - val_loss: 0.6265 - val_accuracy: 0.8870\n",
      "Epoch 2/15\n",
      "1852/1875 [============================>.] - ETA: 0s - loss: 0.5437 - accuracy: 0.9011"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-85a28793d93a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#training del modello\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_flat_zoom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train_flat_zoom_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_flat_zoom_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\MLinPhysics\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\MLinPhysics\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1121\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1123\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1124\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\MLinPhysics\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\MLinPhysics\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\MLinPhysics\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\MLinPhysics\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\miniconda3\\envs\\MLinPhysics\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\MLinPhysics\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\miniconda3\\envs\\MLinPhysics\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\MLinPhysics\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\MLinPhysics\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#epoche di training\n",
    "training_epoch = 15\n",
    "\n",
    "#compilazione del modello\n",
    "model_flat_zoom.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#training del modello\n",
    "history = model_flat_zoom.fit(x=X_train_flat_zoom_int, y=y_train, validation_data=(X_test_flat_zoom_int, y_test), epochs=training_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning del modello e verifica dell'accuratezza sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial non zero weights:  2778  su:  2778\n",
      "\n",
      "peso massimo:  0.972445\n",
      "\n",
      "soglia percentile:  0.0005083537773426735 \tsoglia min W:  0.03125\n",
      "\n",
      "soglia:  0.03125\n",
      "\n",
      "after pruning non zero weights:  1527  su:  2778\n"
     ]
    }
   ],
   "source": [
    "model_flat_zoom_pruned = pruning(model_flat_zoom, minimum_weight=2**-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in log2\n",
      "  \n",
      "C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:824: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:825: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([85., 72., 77., 90., 70., 79., 61., 51., 48., 37., 33., 27.,  9.,\n",
       "        13.,  6.,  5.,  7.,  5.,  1.,  2.]),\n",
       " array([-4.999821  , -4.751846  , -4.50387   , -4.2558947 , -4.0079193 ,\n",
       "        -3.7599437 , -3.5119684 , -3.2639928 , -3.0160172 , -2.7680418 ,\n",
       "        -2.5200663 , -2.272091  , -2.0241153 , -1.7761399 , -1.5281644 ,\n",
       "        -1.2801889 , -1.0322133 , -0.78423786, -0.5362624 , -0.28828692,\n",
       "        -0.04031142], dtype=float32),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_flat_zoom_pruned = np.absolute(get_weights_nparray(model_flat_zoom_pruned))\n",
    "plt.hist(np.log2(w_flat_zoom_pruned[w_flat_zoom_pruned!=0]), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss   Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38490096, 0.9423]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_flat_zoom_pruned.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print('    Loss   Accuracy')\n",
    "model_flat_zoom_pruned.test_on_batch(x=X_test_flat_zoom_int, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvataggio del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'MNIST_flat' + str(size_final) + '_' + str(color_depth) + 'bit'\n",
    "\n",
    "save_model(model_flat_zoom, name_model)\n",
    "save_model(model_flat_zoom_pruned, name_model + '_pruned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvataggio delle immagini test da inserire nella FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista contenente gli indici delle immagini campione nel Test Set del MNIST\n",
    "#num_list[0] contiene l'indice dell'immagine di uno zero, num_list[1] di un uno e così via\n",
    "#           0  1  2  3  4  5  6  7  8  9          \n",
    "num_list = [3, 2, 1, 32,4, 15,21,0, 61,12]\n",
    "\n",
    "\n",
    "#---------------------------------------------------------\n",
    "#--FORMAT OF INPUT IMAGES FOR NN IN FPGA------------------\n",
    "#----------- ap_fixed<7,2> -------------------------------\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# last pixel                 first pixel\n",
    "#[0  0  1  0  0  0  0 | ... | 0  0  0  1  0  0  0]\n",
    "# s  0 -1 -2 -3 -4 -5         s  0 -1 -2 -3 -4 -5 \n",
    "#    2  2  2  2  2  2            2  2  2  2  2  2\n",
    "\n",
    "with open('TESTimg_downto_apfixed7_2.txt', 'w') as f:\n",
    "    for index , num in enumerate(num_list):\n",
    "        f.write(str(index)+ ': ')\n",
    "        for i in np.flip(X_test_flat_zoom_int[num]):\n",
    "            f.write('{:07d}'.format(int((bin(i)[2:]))))\n",
    "        f.write('\\n\\n')\n",
    "        \n",
    "        \n",
    "#---------------------------------------------------------\n",
    "#--FORMAT OF INPUT IMAGES FOR NN IN FPGA------------------\n",
    "#----------- ap_int<7,2> -------------------------------\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# last pixel              first pixel\n",
    "#[0  1  0  0  0  0 | ... | 0  0  0  1  0  0]\n",
    "# s  4  3  2  1  0         s  4  3  2  1  0 \n",
    "#    2  2  2  2  2            2  2  2  2  2  \n",
    "        \n",
    "with open('TESTimg_downto_apint6.txt', 'w') as f:\n",
    "    for index , num in enumerate(num_list):\n",
    "        f.write(str(index)+ ': ')\n",
    "        for i in np.flip(X_test_flat_zoom_int[num]):\n",
    "            f.write('{:06d}'.format(int((bin(i)[2:]))))\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info dettagliate delle inferenze sulle immagini test inserite nella FPGA. \n",
    "Confronto fra modello completo e prunato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predizioni del modello NON PRUNATO sulle immagini di test scelte\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_9_input to have shape (64,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-f6a3c7be0bdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predizioni del modello NON PRUNATO sulle immagini di test scelte\\n\\n\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'number in image: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\nPrediction: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_flat_zoom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_flat_zoom\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[1;31m# generate symbolic tensors).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1060\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m   1061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2651\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    383\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    386\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_9_input to have shape (64,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "print('predizioni del modello NON PRUNATO sulle immagini di test scelte\\n\\n\\n')\n",
    "for i,index in enumerate(num_list):\n",
    "    print('number in image: ', i,'\\nPrediction: ',model_flat_zoom.predict(X_test_flat_zoom_int[index].reshape(1,-1)))\n",
    "    print('\\n')\n",
    "\n",
    "print('predizioni del modello PRUNATO sulle immagini di test scelte\\n\\n\\n')\n",
    "for i,index in enumerate(num_list):\n",
    "    print('number in image: ', i,'\\nPrediction: ',model_flat_zoom_pruned.predict(X_test_flat_zoom_int[index].reshape(1,-1)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni utili per la scrittura del codice vhdl di input.\n",
    "\n",
    "Particolarmente verboso e quindi utile generarlo automaticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista contenente gli indici delle immagini campione nel Test Set del MNIST\n",
    "#num_list[0] contiene l'indice dell'immagine di uno zero, num_list[1] di un uno e così via\n",
    "\n",
    "n_image_test = 100\n",
    "\n",
    "#           0  1  2  3  4  5  6  7  8  9          \n",
    "num_list = [3, 2, 1, 32,4, 15,21,0, 61,12] + [x for x in range(62,62 + n_image_test - 10)]\n",
    "\n",
    "\n",
    "#---------------------------------------------------------\n",
    "#--FORMAT OF INPUT IMAGES FOR NN IN FPGA------------------\n",
    "#----------- ap_fixed<7,2> -------------------------------\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# last pixel                 first pixel\n",
    "#[0  0  1  0  0  0  0 | ... | 0  0  0  1  0  0  0]\n",
    "# s  0 -1 -2 -3 -4 -5         s  0 -1 -2 -3 -4 -5 \n",
    "#    2  2  2  2  2  2            2  2  2  2  2  2\n",
    "\n",
    "'''\n",
    "with open('TESTimg_downto_apfixed7_2.txt', 'w') as f:\n",
    "    for index , num in enumerate(num_list):\n",
    "        f.write(str(index)+ ': ')\n",
    "        for i in np.flip(X_test_flat_zoom_int[num]):\n",
    "            f.write('{:07d}'.format(int((bin(i)[2:]))))\n",
    "        f.write('\\n\\n')\n",
    "'''\n",
    "        \n",
    "#---------------------------------------------------------\n",
    "#--FORMAT OF INPUT IMAGES FOR NN IN FPGA------------------\n",
    "#----------- ap_int<7,2> -------------------------------\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# last pixel              first pixel\n",
    "#[0  1  0  0  0  0 | ... | 0  0  0  1  0  0]\n",
    "# s  4  3  2  1  0         s  4  3  2  1  0 \n",
    "#    2  2  2  2  2            2  2  2  2  2  \n",
    "\n",
    "with open('TESTimg_downto_apint6_100imgs.txt', 'w') as f:\n",
    "    for index , num in enumerate(num_list):\n",
    "        f.write('signal address_' + str(index) + ' : std_logic_vector((size-1) downto 0) := \\\"')\n",
    "        for i in np.flip(X_test_flat_zoom_int[num]):\n",
    "            f.write('{:06d}'.format(int((bin(i)[2:]))))\n",
    "        f.write('\\\";\\n')\n",
    "        \n",
    "\n",
    "#salvataggio dei true label associati alle immagini inserite nella FPGA\n",
    "with open('TESTlabels_downto_apint6_100imgs.txt', 'w') as f:\n",
    "    for index , num in enumerate(num_list):\n",
    "        f.write('signal label_' + str(index) + ' : std_logic_vector(3 downto 0) := \\\"')\n",
    "        f.write('{:04d}'.format(int((bin(y_test_int[num])[2:]))))\n",
    "        f.write('\\\";\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_image_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-96e59c068904>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vhdl_code_1.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'flat_image <= '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_image_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'address_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' when (address = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m') else\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'(others => \\'0\\');'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_image_test' is not defined"
     ]
    }
   ],
   "source": [
    "#generazione di codice vhdl utile per il core di generazione di input nella FPGA\n",
    "\n",
    "with open('vhdl_code_1.txt', 'w') as f:\n",
    "    f.write('flat_image <= ')\n",
    "    for i in range(n_image_test):\n",
    "        f.write('address_' + str(i) + ' when (address = ' + str(i) + ') else\\n')\n",
    "    f.write('(others => \\'0\\');')\n",
    "    \n",
    "with open('vhdl_code_2.txt', 'w') as f:\n",
    "    f.write('true_label <= ')\n",
    "    for i in range(n_image_test):\n",
    "        f.write('label_' + str(i) + ' when (address = ' + str(i) + ') else\\n')\n",
    "    f.write('(others => \\'0\\');')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se si ha già il modello esportato e lo si vuole caricare per fare alcuni test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Giordano\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "load_model = load_model('complete_model_MNIST_flat8_5bit_pruned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34578407, 0.97]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuratezza del modello sulle immagini inserite nella FPGA\n",
    "\n",
    "load_model.test_on_batch(x=X_test_flat_zoom_int[num_list], y=y_test[num_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0019826889038085938 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Calcolo della latenza o tempo di inferenza sulla propria CPU, utilizzando le routine di Keras\n",
    "\n",
    "import time\n",
    "\n",
    "image  = X_test_flat_zoom_int[0].reshape(1,-1)\n",
    "start_time = time.time()\n",
    "\n",
    "load_model.predict(image)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
