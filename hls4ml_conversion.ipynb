{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "###### AE MODEL LOAD ######\n",
    "ae_wo_classifier = load_model('model/AE_model/KERAS_check_best_model.h5')\n",
    "ae_wo_classifier_pruned = load_model('model/AE_model/KERAS_check_pruned_best_model.h5')\n",
    "ae_w_classifier = load_model('model/AE_model/KERAS_check_best_model_classifier.h5')\n",
    "ae_w_classifier_pruned = load_model('model/AE_model/KERAS_check_pruned_best_model_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sparsity\n",
    "Make a quick check that the model was indeed trained sparse. We'll just make a histogram of the weights of the 1st layer, and hopefully observe a large peak in the bin containing '0'. Note logarithmic y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "w = ae_w_classifier_pruned.layers[2].weights[0].numpy()\n",
    "print(w[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of zeros = 0.75\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGbCAYAAACcQnSEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQIElEQVR4nO3dYYzkd13H8c/XXgoJ0RW4CtgWrs011foIcqmIxhAlptAsRcXYPhFM5WxMfX5G4wOfWHzYUEIu0ICJacEmYs8eKSA2fQLYK6G2tVaOpqTXID1osgYfUIs/H+y0LMftdnZ3dv/7vX29kktn/js78/3N3PZ985//ztQYIwDQxU9NPQAAbIZwAdCKcAHQinAB0IpwAdDKgakHSJKDBw+OQ4cOTT0GAHvIww8//N0xxiXnbt8T4Tp06FBOnTo19RgA7CFV9a3zbZ90V2FVLVfV8ZWVlSnHAKCRScM1xjgxxji6tLQ05RgANOLgDABaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWvEmuwC04k12AWjFrkIAWtkTHyQJ+8mhY/f92Pmnb7t+okmgJ8+4AGhFuABoRbgAaEW4AGhFuABoRbgAaEW4AGhFuABoRbgAaEW4AGhFuABoRbgAaEW4AGjFB0kC0IoPkgSgFbsKAWhFuABoRbgAaEW4AGhFuABoRbgAaEW4AGhFuABoRbgAaEW4AGhFuABoRbgAaEW4AGhFuABoRbgAaEW4AGhFuABoRbgAaEW4AGhFuABoRbgAaGXScFXVclUdX1lZmXIMABqZNFxjjBNjjKNLS0tTjgFAI3YVAtCKcAHQinAB0IpwAdCKcAHQinAB0IpwAdCKcAHQinAB0IpwAdCKcAHQinAB0IpwAdCKcAHQinAB0IpwAdCKcAHQinAB0IpwAdCKcAHQinAB0IpwAdCKcAHQinAB0IpwAdCKcAHQinAB0IpwAdCKcAHQinAB0IpwAdCKcAHQinAB0IpwAdCKcAHQinAB0MqBnbjSqnpfkuuT/EyST4wxPr8TtwPA/jP3M66qurOqnquqx87Zfl1VPVlVp6vqWJKMMT47xvhQkluS/P5iRwZgP9vMrsJPJrlu7YaquijJHUneneSaJDdV1TVrLvIXs68DwELMHa4xxoNJnj9n87VJTo8xnhpjvJDk7iQ31KoPJ/ncGONr57u+qjpaVaeq6tTZs2e3Oj8A+8x2D864NMkza86fmW370yTvSvL+qrrlfN84xjg+xjgyxjhyySWXbHMMAPaLHTk4Y4xxe5Lbd+K6AdjftvuM69kkl685f9lsGwDsiO2G66EkV1XVFVV1cZIbk9y7/bEA4Pw2czj8XUm+nOTqqjpTVTePMV5McmuS+5M8keQzY4zHd2ZUANjEa1xjjJvW2X4yycmt3HhVLSdZPnz48Fa+HYB9aNK3fBpjnBhjHF1aWppyDAAa8V6FALQiXAC0IlwAtCJcALQiXAC0Mmm4qmq5qo6vrKxMOQYAjTgcHoBW7CoEoBXhAqAV4QKgFeECoBXhAqAV4QKgFb/HBUArfo8LgFbsKgSgFeECoBXhAqAV4QKgFeECoBXhAqAVv8cFQCt+jwuAVuwqBKAV4QKgFeECoBXhAqAV4QKgFeECoBXhAqAV4QKgFeECoBVv+QRAK97yCYBW7CoEoBXhAqAV4QKgFeECoBXhAqAV4QKgFeECoBXhAqAV4QKgFeECoBXhAqAVb7ILQCveZBeAVuwqBKAV4QKgFeECoBXhAqAV4QKgFeECoBXhAqAV4QKgFeECoBXhAqAV4QKgFeECoBXhAqAV4QKgFeECoBUfJAlAKz5IEoBW7CoEoBXhAqAV4QKgFeECoBXhAqAV4QKgFeECoBXhAqAV4QKgFeECoBXhAqAV4QKgFeECoBXhAqAV4QKgFeECoBXhAqAV4QKgFeECoBXhAqAV4QKglUnDVVXLVXV8ZWVlyjEAaGTScI0xTowxji4tLU05BgCN2FUIQCvCBUArwgVAK8IFQCvCBUArwgVAK8IFQCvCBUArwgVAK8IFQCvCBUArwgVAK8IFQCvCBUArwgVAK8IFQCvCBUArwgVAK8IFQCvCBUArwgVAK8IFQCvCBUArwgVAK8IFQCvCBUArwgVAK8IFQCvCBUArwgVAK8IFQCvCBUArwgVAK8IFQCvCBUArCw9XVV1ZVZ+oqnsWfd0AMFe4qurOqnquqh47Z/t1VfVkVZ2uqmNJMsZ4aoxx804MCwDzPuP6ZJLr1m6oqouS3JHk3UmuSXJTVV2z0OkA4BxzhWuM8WCS58/ZfG2S07NnWC8kuTvJDQueDwB+zHZe47o0yTNrzp9JcmlVvb6qPpbkrVX1Z+t9c1UdrapTVXXq7Nmz2xgDgP3kwKKvcIzxvSS3zHG540mOJ8mRI0fGoucA4MK0nWdczya5fM35y2bbAGDHbCdcDyW5qqquqKqLk9yY5N7FjAUA5zfv4fB3Jflykqur6kxV3TzGeDHJrUnuT/JEks+MMR7fuVEBYM7XuMYYN62z/WSSkwudCAA2MOlbPlXVclUdX1lZmXIMABqZNFxjjBNjjKNLS0tTjgFAI95kF4BWhAuAVoQLgFaEC4BWhAuAVhb+XoWbUVXLSZYPHz485RjADjt07L6XTz992/UTTsKFwOHwALRiVyEArQgXAK0IFwCtCBcArQgXAK04HB5YuLWHv8OiORwegFbsKgSgFeECoBXhAqAV4QKgFeECoBXhAqAV4QKglUnDVVXLVXV8ZWVlyjEAaMQvIAPQil2FALQiXAC0IlwAtCJcALQiXAC0IlwAtCJcALQiXAC0IlwAtHJgyhuvquUky4cPH55yDLbo0LH7fuz807ddP9Ek7JS1j7HHl73CWz4B0IpdhQC0IlwAtCJcALQiXAC0IlwAtCJcALQiXAC0IlwAtCJcALQiXAC0IlwAtOJNdtnQot5I15u1Xnimfkw3c/tTz8pieZNdAFqxqxCAVoQLgFaEC4BWhAuAVoQLgFaEC4BWhAuAVoQLgFaEC4BWhAuAVoQLgFaEC4BWhAuAVoQLgFaEC4BWfJDkNizqQxYvFFN/WN/Ut9/VuX+Pp7TRLBfKY+r/G9vngyQBaMWuQgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFo5MOWNV9VykuXDhw9POcYF49Cx+14+/fRt1084yeasnbuzrvf/vBb1OG31ejb6vnO/ttH9v5nLznv7F+LjvZdN+oxrjHFijHF0aWlpyjEAaMSuQgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWhEuAFoRLgBaES4AWjmw6Cusqtck+WiSF5I8MMb4u0XfBgD711zPuKrqzqp6rqoeO2f7dVX1ZFWdrqpjs82/k+SeMcaHkrx3wfMCsM/Nu6vwk0muW7uhqi5KckeSdye5JslNVXVNksuSPDO72A8XMyYArJorXGOMB5M8f87ma5OcHmM8NcZ4IcndSW5Iciar8Zr7+gFgXtt5jevS/OiZVbIarF9OcnuSj1TV9UlOrPfNVXU0ydEkefOb37yNMX7k0LH7Xj799G3XL+Q6N7qNzVxuq/PMe3uL+r5F2ej2d2O2je7/zTw2i7r/572Ncy+30e1vZu6d+nlY7/amtpl5tnr/T203/n83r93++7bwgzPGGP+T5A/nuNzxJMeT5MiRI2PRcwBwYdrOrrxnk1y+5vxls20AsGO2E66HklxVVVdU1cVJbkxy72LGAoDzm/dw+LuSfDnJ1VV1pqpuHmO8mOTWJPcneSLJZ8YYj+/cqAAw52tcY4yb1tl+MsnJhU4EABuY9HD1qlququMrKytTjgFAI5OGa4xxYoxxdGlpacoxAGjELwgD0IpwAdCKcAHQinAB0IqjCgFoxVGFALRiVyEArQgXAK0IFwCt1BjTfxRWVZ1N8q0Jbvpgku9OcLs7zbp6sa5erGv3vGWMccm5G/dEuKZSVafGGEemnmPRrKsX6+rFuqZnVyEArQgXAK3s93Adn3qAHWJdvVhXL9Y1sX39GhcA/ez3Z1wANCNcALSyr8JVVb9XVY9X1f9V1bqHfVbV01X1aFV9vapO7eaMW7GJdV1XVU9W1emqOrabM25FVb2uqr5QVd+Y/fe161zuh7PH6utVde9uzzmvV7r/q+pVVfXp2de/WlWHJhhzU+ZY0wer6uyax+ePpphzs6rqzqp6rqoeW+frVVW3z9b9b1X1tt2ecSvmWNc7q2plzeP1l7s941zGGPvmT5JfTHJ1kgeSHNngck8nOTj1vItcV5KLknwzyZVJLk7ySJJrpp79Fdb1N0mOzU4fS/LhdS73/alnnWMtr3j/J/mTJB+bnb4xyaennnsBa/pgko9MPesW1vbrSd6W5LF1vv6eJJ9LUknenuSrU8+8oHW9M8k/TT3nK/3ZV8+4xhhPjDGenHqORZtzXdcmOT3GeGqM8UKSu5PcsPPTbcsNST41O/2pJO+bbpRtm+f+X7vee5L8ZlXVLs64WR3/Ts1ljPFgkuc3uMgNSf52rPpKkp+tqjftznRbN8e6WthX4dqEkeTzVfVwVR2depgFuTTJM2vOn5lt28veMMb49uz0fyV5wzqXe3VVnaqqr1TV+3ZntE2b5/5/+TJjjBeTrCR5/a5MtzXz/p363dnutHuq6vLdGW3Hdfx5mtevVNUjVfW5qvqlqYc5nwNTD7BoVfXFJG88z5f+fIzxj3Neza+NMZ6tqp9L8oWq+o/Zv1Qms6B17TkbrWvtmTHGqKr1fnfjLbPH68okX6qqR8cY31z0rGzJiSR3jTF+UFV/nNVnlL8x8Uys72tZ/Xn6flW9J8lnk1w17Ug/6YIL1xjjXQu4jmdn/32uqv4hq7tEJg3XAtb1bJK1/9q9bLZtUhutq6q+U1VvGmN8e7Yb5rl1ruOlx+upqnogyVuz+trLXjLP/f/SZc5U1YEkS0m+tzvjbckrrmmMsXb+j2f1dcsLwZ78edquMcZ/rzl9sqo+WlUHxxh76s137So8R1W9pqp++qXTSX4ryXmPwGnmoSRXVdUVVXVxVl/837NH4M3cm+QDs9MfSPITzyyr6rVV9arZ6YNJfjXJv+/ahPOb5/5fu973J/nSmL1ivke94prOed3nvUme2MX5dtK9Sf5gdnTh25OsrNmt3VZVvfGl11Wr6tqsNmLv/eNp6qNDdvNPkt/O6r7oHyT5TpL7Z9t/PsnJ2ekrs3p01CNJHs/qrrjJZ9/uumbn35PkP7P6bKTDul6f5J+TfCPJF5O8brb9SJKPz06/I8mjs8fr0SQ3Tz33Buv5ifs/yV8lee/s9KuT/H2S00n+NcmVU8+8gDX99ezn6JEk/5LkF6aeec513ZXk20n+d/azdXOSW5LcMvt6Jbljtu5Hs8FRynvpzxzrunXN4/WVJO+Yeubz/fGWTwC0YlchAK0IFwCtCBcArQgXAK0IFwCtCBcArQgXAK38PyjTVa28eUoiAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "w = ae_w_classifier_pruned.layers[2].weights[0].numpy()\n",
    "h, b = np.histogram(w, bins=100)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.bar(b[:-1], h, width=b[1]-b[0])\n",
    "plt.semilogy()\n",
    "print('% of zeros = {}'.format(np.sum(w==0)/np.size(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport plotting\\nfrom plotting import makeRoc\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import accuracy_score\\nfrom tensorflow.keras.models import load_model\\nimport numpy as np\\nmodel = load_model(\\'model/AE_model/KERAS_check_model_w_classifier.h5\\')\\n#model_pruned =load_model(\\'model/AE_model/KERAS_check_pruned_model_w_classifier.h5\\')\\n\\ny_test = data_zoom.y_test\\n\\ny_ref = model.predict(data_zoom.x_test)\\n#y_prune = ae_wo_classifier.predict(data_zoom.x_test)\\n#print(\"Accuracy unpruned: {}\".format(accuracy_score(np.argmax(y_test.reshape(10000,10), axis=1), np.argmax(y_ref.reshape((10000,10)), axis=1))))\\n#print(\"Accuracy pruned:   {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_prune, axis=1))))\\n\\nfig, ax = plt.subplots(figsize=(9, 9))\\n_ = makeRoc(y_test, y_ref)\\nplt.gca().set_prop_cycle(None) # reset the colors\\n_ = plotting.makeRoc(y_test, y_prune, linestyle=\\'--\\')\\n\\nfrom matplotlib.lines import Line2D\\nlines = [Line2D([0], [0], ls=\\'-\\'),\\n         Line2D([0], [0], ls=\\'--\\')]\\nfrom matplotlib.legend import Legend\\nleg = Legend(ax, lines, labels=[\\'unpruned\\', \\'pruned\\'],\\n            loc=\\'lower right\\', frameon=False)\\nax.add_artist(leg)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import MNIST_dataset as mnist\n",
    "size_final = 10\n",
    "data_zoom = mnist.MNISTData(size_final=size_final, color_depth=5)\n",
    "\"\"\"\n",
    "import plotting\n",
    "from plotting import makeRoc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "model = load_model('model/AE_model/KERAS_check_model_w_classifier.h5')\n",
    "#model_pruned =load_model('model/AE_model/KERAS_check_pruned_model_w_classifier.h5')\n",
    "\n",
    "y_test = data_zoom.y_test\n",
    "\n",
    "y_ref = model.predict(data_zoom.x_test)\n",
    "#y_prune = ae_wo_classifier.predict(data_zoom.x_test)\n",
    "#print(\"Accuracy unpruned: {}\".format(accuracy_score(np.argmax(y_test.reshape(10000,10), axis=1), np.argmax(y_ref.reshape((10000,10)), axis=1))))\n",
    "#print(\"Accuracy pruned:   {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_prune, axis=1))))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "_ = makeRoc(y_test, y_ref)\n",
    "plt.gca().set_prop_cycle(None) # reset the colors\n",
    "_ = plotting.makeRoc(y_test, y_prune, linestyle='--')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "lines = [Line2D([0], [0], ls='-'),\n",
    "         Line2D([0], [0], ls='--')]\n",
    "from matplotlib.legend import Legend\n",
    "leg = Legend(ax, lines, labels=['unpruned', 'pruned'],\n",
    "            loc='lower right', frameon=False)\n",
    "ax.add_artist(leg)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make an hls4ml config & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: encoder_input, layer type: Input\n",
      "Layer name: dense_32, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_32\n",
      "Layer name: dense_33, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_33\n",
      "Layer name: encoder_output, layer type: Dense\n",
      "  -> Activation (relu), layer name: encoder_output\n",
      "Layer name: dense_34, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_34\n",
      "Layer name: dense_36, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_36\n",
      "Layer name: dense_35, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_35\n",
      "Layer name: dense_37, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_37\n",
      "Layer name: ecoder_output, layer type: Dense\n",
      "  -> Activation (sigmoid), layer name: ecoder_output\n",
      "Layer name: classifier_output, layer type: Dense\n",
      "  -> Activation (softmax), layer name: classifier_output\n",
      "-----------------------------------\n",
      "Configuration\n",
      "{'Model': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Latency'}}\n",
      "-----------------------------------\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: encoder_input, layer type: InputLayer, input shapes: [[None, 64]], output shape: [None, 64]\n",
      "Layer name: dense_32, layer type: Dense, input shapes: [[None, 64]], output shape: [None, 32]\n",
      "Layer name: dense_33, layer type: Dense, input shapes: [[None, 32]], output shape: [None, 16]\n",
      "Layer name: encoder_output, layer type: Dense, input shapes: [[None, 16]], output shape: [None, 2]\n",
      "Layer name: dense_34, layer type: Dense, input shapes: [[None, 2]], output shape: [None, 16]\n",
      "Layer name: dense_36, layer type: Dense, input shapes: [[None, 2]], output shape: [None, 16]\n",
      "Layer name: dense_35, layer type: Dense, input shapes: [[None, 16]], output shape: [None, 32]\n",
      "Layer name: dense_37, layer type: Dense, input shapes: [[None, 16]], output shape: [None, 32]\n",
      "Layer name: ecoder_output, layer type: Dense, input shapes: [[None, 32]], output shape: [None, 64]\n",
      "Layer name: classifier_output, layer type: Dense, input shapes: [[None, 32]], output shape: [None, 10]\n",
      "Creating HLS model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loren\\miniconda3\\lib\\site-packages\\hls4ml\\converters\\__init__.py:16: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\")\n"
     ]
    }
   ],
   "source": [
    "#import plotting\n",
    "import hls4ml\n",
    "config = hls4ml.utils.config_from_keras_model(ae_w_classifier_pruned, granularity='model')\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Configuration\")\n",
    "print(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(ae_w_classifier_pruned,\n",
    "                                                       hls_config=config,\n",
    "                                                       output_dir='model/AE_model/hls4ml_prj',\n",
    "                                                       part='xc250-figd2104-2L-e')\n",
    "                                                       #part='xcvu9p-flgc2104aaz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\loren\\\\AppData\\\\Local\\\\Temp\\\\tmpz324c0_b.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ea3840741460>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhls4ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhls_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_precision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\hls4ml\\utils\\plot.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, show_precision, rankdir, dpi)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, path, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1816\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1818\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1819\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1820\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\loren\\\\AppData\\\\Local\\\\Temp\\\\tmpz324c0_b.png'"
     ]
    }
   ],
   "source": [
    "hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(csim=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "789093bb736187c46b4a361b2c83e244a48ff9bb7ad40e66e5ffb40455c8edff"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
